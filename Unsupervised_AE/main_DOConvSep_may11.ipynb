{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oTBwyod2dscE",
    "outputId": "3d3b1a18-e036-4a13-f063-71217b20b8a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONHASHSEED=0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn import init\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import codecs\n",
    "\n",
    "%matplotlib inline\n",
    "%env PYTHONHASHSEED=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjbdJvo6dscO"
   },
   "source": [
    "# Function Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABUGOSoYdscP"
   },
   "source": [
    "## Word2Vec model\n",
    "Code from [ABAE GitHub](https://github.com/KirillKrasikov/Attention-And-Capsule-Based-Aspect-Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "a6rjYQxjdscR"
   },
   "outputs": [],
   "source": [
    "def parse_xml(path):\n",
    "    root = ET.parse(path).getroot()\n",
    "    return root\n",
    "\n",
    "\n",
    "def get_text_from_xml(xml):\n",
    "    texts = []\n",
    "    texts_merged = []\n",
    "\n",
    "    labels = []\n",
    "    labels_merged = []\n",
    "\n",
    "    for review in xml.findall('Review'):\n",
    "        _texts = ''\n",
    "        _labels_all = {}\n",
    "\n",
    "        for sentence in review.find('sentences').findall('sentence'):\n",
    "            text = sentence.find('text').text\n",
    "            texts.append(text)\n",
    "            _texts += ' {}'.format(text)\n",
    "\n",
    "\n",
    "            op = sentence.find('Opinions')\n",
    "            if op is not None:\n",
    "                _labels = {}\n",
    "                for label in sentence.find('Opinions').findall('Opinion'):\n",
    "                    target_term = label.get('target')\n",
    "                    target_aspect = label.get('category')\n",
    "                    _labels[target_aspect] = target_term\n",
    "\n",
    "                labels.append(_labels)\n",
    "                _labels_all.update(_labels)\n",
    "            else:\n",
    "                labels.append({})\n",
    "\n",
    "        texts_merged.append(_texts)\n",
    "        labels_merged.append(_labels_all)\n",
    "\n",
    "    return texts, texts_merged, labels, labels_merged\n",
    "\n",
    "def parseSentence(line):\n",
    "    lmtzr = WordNetLemmatizer()    \n",
    "    stop = stopwords.words('english')\n",
    "    text_token = CountVectorizer().build_tokenizer()(line.lower())\n",
    "    text_rmstop = [i for i in text_token if i not in stop]\n",
    "    text_stem = [lmtzr.lemmatize(w) for w in text_rmstop]\n",
    "    return text_stem\n",
    "\n",
    "def preprocess_train(train_path, preprocessed_fold):\n",
    "    f = codecs.open(train_path, 'r', 'utf-8')\n",
    "    out = codecs.open(os.path.join(preprocessed_fold, 'train.txt'), 'w', 'utf-8')\n",
    "\n",
    "    for line in f:\n",
    "        tokens = parseSentence(line)\n",
    "        if len(tokens) > 0:\n",
    "            out.write(' '.join(tokens)+'\\n')\n",
    "\n",
    "def preprocess_test(args, test_path, test_lab_path, preprocessed_fold):\n",
    "    # only keep sentences with single aspect label that are defined in args.labels\n",
    "    if args.domain == 'laptop':\n",
    "        root = parse_xml(args.raw_test_xml_path)\n",
    "        texts, texts_merged, labels, labels_merged = get_text_from_xml(root)\n",
    "        gold_labels = process_labels_semeval_laptop(labels)\n",
    "        with open(test_path, 'w') as f:\n",
    "            f.writelines([comment + '\\n' for comment in texts])\n",
    "        with open(test_lab_path, 'w') as f:\n",
    "            f.writelines([label + '\\n' for label in gold_labels])\n",
    "        \n",
    "    f1 = codecs.open(test_path, 'r', 'utf-8')\n",
    "    f2 = codecs.open(test_lab_path, 'r', 'utf-8')\n",
    "\n",
    "    out1 = codecs.open(os.path.join(preprocessed_fold, 'test.txt'), 'w', 'utf-8')\n",
    "    out2 = codecs.open(os.path.join(preprocessed_fold, 'test_label.txt'), 'w', 'utf-8')\n",
    "\n",
    "    for text, label in zip(f1, f2):\n",
    "        label = label.strip()\n",
    "        if label not in args.aspects:\n",
    "            continue\n",
    "        tokens = parseSentence(text)\n",
    "        if len(tokens) > 0:\n",
    "            out1.write(' '.join(tokens) + '\\n')\n",
    "            out2.write(label+'\\n')\n",
    "\n",
    "\n",
    "def preprocess(args, train_path, test_path, test_lab_path, preprocessed_fold):\n",
    "    if not os.path.exists(preprocessed_fold):\n",
    "        os.makedirs(preprocessed_fold)\n",
    "        print('Folder \"{}\" created!'.format(preprocessed_fold))\n",
    "    print(\"Processing train data!\")\n",
    "    preprocess_train(train_path, preprocessed_fold)\n",
    "    print(\"Processing test data!\")\n",
    "    preprocess_test(args, test_path, test_lab_path, preprocessed_fold)\n",
    "    \n",
    "# W2V training\n",
    "class MySentences(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in codecs.open(self.filename, 'r', 'utf-8'):\n",
    "            yield line.split()\n",
    "\n",
    "\n",
    "def word2vec(train_path, w2v_path, size=200, window=5, min_count=10):\n",
    "    sentences = MySentences(train_path)\n",
    "    model = gensim.models.Word2Vec(sentences, size=size, window=window, min_count=min_count, workers=1)\n",
    "    model.save(w2v_path)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdwCoXQPdscT"
   },
   "source": [
    "## DOConv Layer\n",
    "Code from [DOConv GitHub](https://github.com/yangyanli/DO-Conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8TTD2j-BdscU"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import init\n",
    "from itertools import repeat\n",
    "from torch.nn import functional as F\n",
    "from torch._six import container_abcs\n",
    "from torch._jit_internal import Optional\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "\n",
    "class DOConv2d(Module):\n",
    "    \"\"\"\n",
    "       DOConv2d can be used as an alternative for torch.nn.Conv2d.\n",
    "       The interface is similar to that of Conv2d, with one exception:\n",
    "            1. D_mul: the depth multiplier for the over-parameterization.\n",
    "       Note that the groups parameter switchs between DO-Conv (groups=1),\n",
    "       DO-DConv (groups=in_channels), DO-GConv (otherwise).\n",
    "    \"\"\"\n",
    "    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n",
    "                     'padding_mode', 'output_padding', 'in_channels',\n",
    "                     'out_channels', 'kernel_size', 'D_mul']\n",
    "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, D_mul=None, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'):\n",
    "        super(DOConv2d, self).__init__()\n",
    "\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = _pair(stride)\n",
    "        padding = _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "        if padding_mode not in valid_padding_modes:\n",
    "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
    "                valid_padding_modes, padding_mode))\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.padding_mode = padding_mode\n",
    "        self._padding_repeated_twice = tuple(x for x in self.padding for _ in range(2))\n",
    "\n",
    "        #################################### Initailization of D & W ###################################\n",
    "        M = self.kernel_size[0]\n",
    "        N = self.kernel_size[1]\n",
    "        self.D_mul = M * N if D_mul is None or M * N <= 1 else D_mul\n",
    "        self.W = Parameter(torch.Tensor(out_channels, in_channels // groups, self.D_mul))\n",
    "        init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "        if M * N > 1:\n",
    "            self.D = Parameter(torch.Tensor(in_channels, M * N, self.D_mul))\n",
    "            init_zero = np.zeros([in_channels, M * N, self.D_mul], dtype=np.float32)\n",
    "            self.D.data = torch.from_numpy(init_zero)\n",
    "\n",
    "            eye = torch.reshape(torch.eye(M * N, dtype=torch.float32), (1, M * N, M * N))\n",
    "            D_diag = eye.repeat((in_channels, 1, self.D_mul // (M * N)))\n",
    "            if self.D_mul % (M * N) != 0:  # the cases when D_mul > M * N\n",
    "                zeros = torch.zeros([in_channels, M * N, self.D_mul % (M * N)])\n",
    "                self.D_diag = Parameter(torch.cat([D_diag, zeros], dim=2), requires_grad=False)\n",
    "            else:  # the case when D_mul = M * N\n",
    "                self.D_diag = Parameter(D_diag, requires_grad=False)\n",
    "        ##################################################################################################\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.W)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        if self.padding_mode != 'zeros':\n",
    "            s += ', padding_mode={padding_mode}'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(DOConv2d, self).__setstate__(state)\n",
    "        if not hasattr(self, 'padding_mode'):\n",
    "            self.padding_mode = 'zeros'\n",
    "\n",
    "    def _conv_forward(self, input, weight):\n",
    "        if self.padding_mode != 'zeros':\n",
    "            return F.conv2d(F.pad(input, self._padding_repeated_twice, mode=self.padding_mode),\n",
    "                            weight, self.bias, self.stride,\n",
    "                            _pair(0), self.dilation, self.groups)\n",
    "        return F.conv2d(input, weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input):\n",
    "        M = self.kernel_size[0]\n",
    "        N = self.kernel_size[1]\n",
    "        DoW_shape = (self.out_channels, self.in_channels // self.groups, M, N)\n",
    "        if M * N > 1:\n",
    "            ######################### Compute DoW #################\n",
    "            # (input_channels, D_mul, M * N)\n",
    "            D = self.D + self.D_diag\n",
    "            W = torch.reshape(self.W, (self.out_channels // self.groups, self.in_channels, self.D_mul))\n",
    "\n",
    "            # einsum outputs (out_channels // groups, in_channels, M * N),\n",
    "            # which is reshaped to\n",
    "            # (out_channels, in_channels // groups, M, N)\n",
    "            DoW = torch.reshape(torch.einsum('ims,ois->oim', D, W), DoW_shape)\n",
    "            #######################################################\n",
    "        else:\n",
    "            # in this case D_mul == M * N\n",
    "            # reshape from\n",
    "            # (out_channels, in_channels // groups, D_mul)\n",
    "            # to\n",
    "            # (out_channels, in_channels // groups, M, N)\n",
    "            DoW = torch.reshape(self.W, DoW_shape)\n",
    "        return self._conv_forward(input, DoW)\n",
    "\n",
    "\n",
    "def _ntuple(n):\n",
    "    def parse(x):\n",
    "        if isinstance(x, container_abcs.Iterable):\n",
    "            return x\n",
    "        return tuple(repeat(x, n))\n",
    "\n",
    "    return parse\n",
    "\n",
    "\n",
    "_pair = _ntuple(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsI1xQsYdscW"
   },
   "source": [
    "## Data Vectorization classes\n",
    "Code from [ABAE GitHub](https://github.com/KirillKrasikov/Attention-And-Capsule-Based-Aspect-Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CEij93TDdscW"
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\" Class to process text and extract vocabulary for mapping\n",
    "    \n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "            mask_token (str): the MASK token to add into the Vocabulary; indicates\n",
    "                a position that will not be used in updating the model's parameters\n",
    "            add_unk (bool): a flag that indicates whether to add the UNK token\n",
    "            unk_token (str): the UNK token to add into the Vocabulary\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, token_to_idx=None, mask_token=\"<MASK>\", unk_token=\"<UNK>\", num_token='<NUM>'):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "\n",
    "        self._unk_token = unk_token\n",
    "        self._mask_token = mask_token\n",
    "        self._num_token = num_token\n",
    "        \n",
    "        self.unk_index = self.add_token(unk_token) \n",
    "        self.num_index = self.add_token(num_token)\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        \"\"\"Returns a dictionary that can be serialized\"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx, \n",
    "                'add_unk': self._add_unk, \n",
    "                'unk_token': self._unk_token, \n",
    "                'mask_token': self._mask_token,\n",
    "                'num_token': self._num_token,\n",
    "               }\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"Instantiates the Vocabulary from a serialized dictionary\"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "\n",
    "        return index\n",
    "            \n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"Add a list of tokens into the Vocabulary\n",
    "        \n",
    "        Args:\n",
    "            tokens (list): a list of string tokens\n",
    "\n",
    "        Returns:\n",
    "            indices (list): a list of indices corresponding to the tokens\n",
    "\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token or the UNK index if token isn't present\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "\n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) for the UNK functionality \n",
    "\n",
    "        \"\"\"\n",
    "        if self.is_number(token):\n",
    "            return self.num_index\n",
    "            \n",
    "        return self._token_to_idx.get(token, self.unk_index)\n",
    "    \n",
    "    def is_number(self, token):\n",
    "        \"\"\"Returns true if token in number else false\"\"\"\n",
    "        num_regex = re.compile('^[+-]?[0-9]+\\.?[0-9]*$')\n",
    "\n",
    "        return bool(num_regex.match(token))\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "class Vectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\n",
    "    \n",
    "        Args:\n",
    "            vocab (Vocabulary): maps words to integers\n",
    "\n",
    "    \"\"\"    \n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def vectorize(self, context, vector_length=-1):\n",
    "        \"\"\"Vectorizer\n",
    "\n",
    "        Args:\n",
    "            context (str): the string of words separated by a space\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        indices = [self.vocab.lookup_token(token) for token in context.split(' ')]\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "        \n",
    "        if vector_length >= len(indices):\n",
    "            out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "            out_vector[-len(indices):] = indices\n",
    "            out_vector[:-len(indices)] = self.vocab.mask_index\n",
    "        \n",
    "        else:\n",
    "            out_vector = np.array(indices[:vector_length], dtype=np.int64)\n",
    "            \n",
    "        return out_vector\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            df(pandas.DataFrame): the target dataset\n",
    "\n",
    "        Returns:\n",
    "            an instance of the Vectorizer\n",
    "\n",
    "        \"\"\"\n",
    "        vocab = Vocabulary()\n",
    "        for index, row in df.iterrows():\n",
    "            for token in row.context.split(' '):\n",
    "                vocab.add_token(token)\n",
    "            vocab.add_token(row.target)\n",
    "            \n",
    "        return cls(vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        vocab = Vocabulary.from_serializable(contents['vocab'])\n",
    "        return cls(vocab=vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'vocab': self.vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAUVrq3PdscW"
   },
   "source": [
    "### The Dataset\n",
    "Code from [ABAE GitHub](https://github.com/KirillKrasikov/Attention-And-Capsule-Based-Aspect-Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NMMoLW5XdscX"
   },
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    \"\"\" Dataset reader\n",
    "\n",
    "        Args:\n",
    "            df(pandas.DataFrame): the dataset\n",
    "            vectorizer (Vectorizer): vectorizer instatiated from dataset\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, train_df, test_df, vectorizer, max_length):\n",
    "\n",
    "        self.train_df = train_df\n",
    "        self.train_size = len(train_df)\n",
    "        \n",
    "        self.test_df = test_df\n",
    "        self.test_size = len(test_df)\n",
    "        \n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        \n",
    "        if max_length < 0:\n",
    "            self._max_seq_length = max(map(measure_len, train_df.sentence))\n",
    "        else:\n",
    "            self._max_seq_length = max_length\n",
    "        \n",
    "        self._lookup_dict = {\n",
    "            'train': (self.train_df, self.train_size),\n",
    "            'test': (self.test_df, self.test_size)\n",
    "        }\n",
    "        \n",
    "        self.set_split('train')\n",
    "        \n",
    "    @property\n",
    "    def max_seq_length(self):\n",
    "        \"\"\"Max dataset sequence len\"\"\"\n",
    "        return self._max_seq_length\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\"Selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        \"\"\"Returns the vectorizer\"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "\n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "        target = 0.\n",
    "\n",
    "        context_vector = self._vectorizer.vectorize(row.sentence, self._max_seq_length)\n",
    "\n",
    "        return {\n",
    "            'x_data': context_vector,\n",
    "            'y_target': 0. if self._target_split == 'train' else row.label,\n",
    "        }\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97VJ1hiZdscZ"
   },
   "source": [
    "### Utils\n",
    "Code from [ABAE GitHub](https://github.com/KirillKrasikov/Attention-And-Capsule-Based-Aspect-Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "odLPxv7UdscZ"
   },
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"A generator function which wraps the PyTorch DataLoader. It will ensure \n",
    "        each tensor is on the write device location\n",
    "        \n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device) \\\n",
    "            if isinstance(data_dict[name], torch.Tensor) else data_dict[name]\n",
    "        yield out_data_dict\n",
    "        \n",
    "def preprocess_text(text):\n",
    "    \"\"\"Text preprocessing regular expression\"\"\"\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text\n",
    "\n",
    "def get_centroids(w2v_model, aspects_count):\n",
    "    \"\"\"Clustering all word vectors with K-means and returning L2-normalizes\n",
    "        cluster centroids; used for aspects matrix initialization\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=aspects_count, verbose=0, n_init=100)\n",
    "    m = []\n",
    "\n",
    "    for k in w2v_model.wv.vocab:\n",
    "        m.append(w2v_model.wv[k])\n",
    "\n",
    "    m = np.matrix(m)\n",
    "\n",
    "    km.fit(m)\n",
    "    clusters = km.cluster_centers_\n",
    "\n",
    "    # L2 normalization\n",
    "    norm_aspect_matrix = clusters / np.linalg.norm(clusters, axis=-1, keepdims=True)\n",
    "\n",
    "    return norm_aspect_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7DIGNK-dsca"
   },
   "source": [
    "# Attentions\n",
    "SelfAttentionDOConv` class code is from [ABAE GitHub](https://github.com/KirillKrasikov/Attention-And-Capsule-Based-Aspect-Extraction) modified based on [DOConv GitHub](https://github.com/yangyanli/DO-Conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "JWzqk6ZTdsca"
   },
   "outputs": [],
   "source": [
    "class SelfAttentionDOConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional multidimensial attention with DOConv Layers\n",
    "    Args:\n",
    "        wv_dim: word vector sizeluence\n",
    "        maxlen: sentence max length taken into account\n",
    "        asp_count: aspect number\n",
    "    \"\"\"\n",
    "    def __init__(self, wv_dim, maxlen, asp_count):\n",
    "        super(SelfAttentionDOConv, self).__init__()\n",
    "        self.wv_dim = wv_dim\n",
    "\n",
    "        self.maxlen = maxlen\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.attention_softmax = torch.nn.Softmax(dim=1)\n",
    "        self.asp_count = asp_count\n",
    "\n",
    "        # groups number equal to aspect number allows keeping the number of channel\n",
    "        # one aspect - one attention channel\n",
    "        self.conv1_3 = DOConv2d(self.asp_count, self.asp_count, kernel_size=(3, self.wv_dim), stride=(1, 1), padding=(1, 0), dilation=(1, 1), groups=self.asp_count)\n",
    "        self.conv1_5 = DOConv2d(self.asp_count, self.asp_count, kernel_size=(5, self.wv_dim), stride=(1, 1), padding=(2, 0), dilation=(1, 1), groups=self.asp_count)\n",
    "        self.conv1_7 = DOConv2d(self.asp_count, self.asp_count, kernel_size=(7, self.wv_dim), stride=(1, 1), padding=(3, 0), dilation=(1, 1), groups=self.asp_count)\n",
    "        self.conv1_1 = DOConv2d(self.asp_count, self.asp_count, kernel_size=(1, self.wv_dim), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=self.asp_count)\n",
    "\n",
    "    def forward(self, input_embeddings): \n",
    "        # expend dimensions of input data for each aspect\n",
    "        mean_embedding = torch.mean(input_embeddings, (1,)).unsqueeze(2)    \n",
    "        input_embeddings = input_embeddings.unsqueeze(1).repeat(1, self.asp_count, 1, 1)\n",
    "        # calculate convolutions\n",
    "        C_1 = self.conv1_1(input_embeddings)\n",
    "        C_3 = self.conv1_3(input_embeddings)\n",
    "        C_5 = self.conv1_5(input_embeddings)\n",
    "        C_7 = self.conv1_7(input_embeddings)\n",
    "\n",
    "        # concatinate convolutions and take mean to give one number for each word in the sentence\n",
    "        results = torch.cat([C_1, C_3, C_5, C_7], -1).mean(-1)\n",
    "        results = results.mean(1)\n",
    "        # Use softmax activation to get attention probabilities \n",
    "        results = self.attention_softmax(self.tanh(results)) # imported  \n",
    "        return results\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return 'wv_dim={}, maxlen={}'.format(self.wv_dim, self.maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Dc4F4w4dsca"
   },
   "source": [
    "# The model\n",
    "Code from [ABAE GitHub](https://github.com/KirillKrasikov/Attention-And-Capsule-Based-Aspect-Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "m5OVVn7Ddscc"
   },
   "outputs": [],
   "source": [
    "class ABAE(torch.nn.Module):\n",
    "    \"\"\" The model described in the paper ``An Unsupervised Neural Attention Model for Aspect Extraction''\n",
    "        by He, Ruidan and  Lee, Wee Sun  and  Ng, Hwee Tou  and  Dahlmeier, Daniel, ACL2017\n",
    "        https://aclweb.org/anthology/papers/P/P17/P17-1036/. \n",
    "        \n",
    "        Based on implementation by Anton Alekseev: ''https://github.com/alexeyev/abae-pytorch''. \n",
    "        Changes: \n",
    "        - removed ortho regularization;\n",
    "        - changed activation function;\n",
    "        - embedding added in model;\n",
    "        - added tanh function to attention output;\n",
    "        - added encoder output flag.\n",
    "        \n",
    "        Args:\n",
    "            wv_dim: word vector size\n",
    "            asp_count: number of aspects\n",
    "            ortho_reg: coefficient for tuning the ortho-regularizer's influence\n",
    "            maxlen: sentence max length taken into account\n",
    "            init_aspects_matrix: None or init. matrix for aspects\n",
    "            pretrained_embedding: w2v vectors\n",
    "            encoder_only: bool - return output after encoding\n",
    "            padding_index: Mask index\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        wv_dim, \n",
    "        asp_count,\n",
    "        maxlen, \n",
    "        init_aspects_matrix,\n",
    "        pretrained_embedding,\n",
    "        padding_index,\n",
    "        encoder_only=False,\n",
    "        attention_mech = 'abae',\n",
    "    ):\n",
    "        super(ABAE, self).__init__()\n",
    "        self.wv_dim = wv_dim\n",
    "        self.asp_count = asp_count\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(pretrained_embedding), \n",
    "            padding_idx=padding_index,\n",
    "        )\n",
    "        \n",
    "        self.attention_mech = attention_mech\n",
    "        if self.attention_mech == 'DOConv':\n",
    "            self.attention = SelfAttentionDOConv(wv_dim, maxlen, asp_count) #DOConv attention         \n",
    "        \n",
    "        self.linear_transform = torch.nn.Linear(self.wv_dim, self.asp_count)\n",
    "        self.softmax_aspects = torch.nn.Softmax(dim=1)\n",
    "        self.aspects_embeddings = Parameter(torch.empty(size=(wv_dim, asp_count)))\n",
    "\n",
    "        if init_aspects_matrix is None:\n",
    "            torch.nn.init.xavier_uniform(self.aspects_embeddings)\n",
    "        else:\n",
    "            self.aspects_embeddings.data = torch.from_numpy(init_aspects_matrix.T)\n",
    "            \n",
    "        self.encoder_only = encoder_only\n",
    "\n",
    "        \n",
    "        self.ortho = 0.1 #ortogonal regularisation\n",
    "        \n",
    "        \n",
    "        print('===================================')\n",
    "        print('Training/Evaluating using {}'.format(self.attention_mech))\n",
    "\n",
    "        \n",
    "    def get_aspects_importances(self, text_embeddings):\n",
    "        \"\"\"Get aspect importances\n",
    "        \n",
    "        Args:\n",
    "            text_embedding: embeddings of a sentence as input\n",
    "        \n",
    "        Returns: \n",
    "            attention weights, aspects_importances, weighted_text_emb\n",
    "\n",
    "        \"\"\"\n",
    "        # compute attention scores, looking at text embeddings average\n",
    "        attention_weights = self.attention(text_embeddings)\n",
    "        # multiplying text embeddings by attention scores -- and summing\n",
    "        # (matmul: we sum every word embedding's coordinate with attention weights)\n",
    "        weighted_text_emb = torch.matmul(attention_weights.unsqueeze(1),  # (batch, 1, sentence)\n",
    "                                         text_embeddings  # (batch, sentence, wv_dim)\n",
    "                                         ).squeeze()   \n",
    "        assert (self.attention_mech in ['DOConv']), \"Cound not understand attention_mech: {}. It should ne: abae, cmam, XSepConv, DOConv, XSepDOConv\".format(self.attention_mech)\n",
    "\n",
    "        # encoding with a simple feed-forward layer (wv_dim) -> (aspects_count)\n",
    "        raw_importances = self.linear_transform(weighted_text_emb)\n",
    "\n",
    "        # computing 'aspects distribution in a sentence'\n",
    "        aspects_importances = self.softmax_aspects(raw_importances)\n",
    "\n",
    "        return attention_weights, aspects_importances, weighted_text_emb\n",
    "\n",
    "    def forward(self, text_embeddings, negative_samples_texts):\n",
    "        \n",
    "        text_embeddings = self.embedding(text_embeddings)\n",
    "\n",
    "        # encoding: words embeddings -> sentence embedding, aspects importances\n",
    "        attention_weights, aspects_importances, weighted_text_emb = self.get_aspects_importances(text_embeddings)\n",
    "        \n",
    "        if self.encoder_only:\n",
    "            return aspects_importances, attention_weights\n",
    "        else:\n",
    "            negative_samples_texts = self.embedding(negative_samples_texts)\n",
    "            \n",
    "            # negative samples are averaged\n",
    "            averaged_negative_samples = torch.mean(negative_samples_texts, dim=1)\n",
    "            averaged_negative_samples = torch.mean(averaged_negative_samples, dim=1)\n",
    "            \n",
    "            # decoding: aspects embeddings matrix, aspects_importances -> recovered sentence embedding\n",
    "            recovered_emb = torch.matmul(self.aspects_embeddings, aspects_importances.unsqueeze(2)).squeeze()\n",
    "              \n",
    "            return weighted_text_emb, recovered_emb, averaged_negative_samples, self.aspects_embeddings.t()\n",
    "\n",
    "    def get_aspect_words(self, w2v_model, topn=10):\n",
    "        \"\"\"Getting aspects words\"\"\"\n",
    "        words = []\n",
    "        aspects = self.aspects_embeddings.cpu().detach().numpy()\n",
    "        words_scores = w2v_model.wv.vectors.dot(aspects)\n",
    "\n",
    "        for row in range(aspects.shape[1]):\n",
    "            argmax_scalar_products = np.argsort(- words_scores[:, row])[:topn]\n",
    "            words.append([w2v_model.wv.index2word[i] for i in argmax_scalar_products])\n",
    "\n",
    "        return words\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bawZaHSmdscf"
   },
   "source": [
    "### Training utils\n",
    "Code from [ABAE GitHub](https://github.com/KirillKrasikov/Attention-And-Capsule-Based-Aspect-Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "3hceCwUydscf"
   },
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "        \n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "    \n",
    "def make_train_state(args):\n",
    "    return {\n",
    "        'stop_early': False,\n",
    "        'early_stopping_step': 0,\n",
    "        'early_stopping_best_val': 1e8,\n",
    "        'learning_rate': args.learning_rate,\n",
    "        'epoch_index': 0,\n",
    "        'train_loss': [],\n",
    "        'model_filename': os.path.join(args.save_dir, args.model_state_file)\n",
    "    }\n",
    "\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    Args:\n",
    "        args: main arguments\n",
    "        model: model to train\n",
    "        train_state: a dictionary representing the training state values\n",
    "    \n",
    "    Returns:\n",
    "        new train_state\n",
    "\n",
    "    \"\"\"\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['train_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUWA-BBVdscf"
   },
   "source": [
    "# Settings\n",
    "Code from [ABAE GitHub](https://github.com/KirillKrasikov/Attention-And-Capsule-Based-Aspect-Extraction), some parameters are added for new attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PWrSWUNOdscf",
    "outputId": "1315c955-d874-4326-d093-f3a1b8026c6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "base = './'\n",
    "args = Namespace(\n",
    "    # raw data path definition\n",
    "    domain = 'restaurant',\n",
    "    raw_train_path=base+'data/restaurant/train.txt',\n",
    "    raw_test_path=base+'data/restaurant/test.txt',\n",
    "    raw_test_lab_path=base+'data/restaurant/test_label.txt',\n",
    "    preprocessed_fold=base+'preprocessed_data/restaurant/',\n",
    "    \n",
    "    # preprocessed data path definition.\n",
    "    train_data=base+'preprocessed_data/restaurant/train.txt',\n",
    "    test_data=base+'preprocessed_data/restaurant/test.txt',\n",
    "    test_labels=base+'preprocessed_data/restaurant/test_label.txt',\n",
    "    emb_path=base+'preprocessed_data/restaurant/w2v_embedding',\n",
    "    aspects=['Food', 'Staff', 'Ambience'],  # aspects for restaurant \n",
    "\n",
    "\n",
    "    #word2vec params\n",
    "    window=5, \n",
    "    min_count=10, \n",
    "    emb_dim=200,\n",
    "    \n",
    "    vocab_size=9000,\n",
    "    aspect_size=14,\n",
    "    \n",
    "    \n",
    "    # training params  \n",
    "    batch_size=50,\n",
    "    epochs=15,\n",
    "    neg_size=20,\n",
    "    maxlen=35, # -1 means no limit, set to 15-20 for convolutional attentions\n",
    "\n",
    "    #losses\n",
    "    tripletmargin=1,\n",
    "\n",
    "    cuda=True,\n",
    "    reload_from_files=False,\n",
    "    learning_rate=1e-3,\n",
    "    early_stopping_criteria=5,  \n",
    "    catch_keyboard_interrupt=True,\n",
    "    seed=1234,\n",
    "    attention_mech = 'DOConv', # 'abae', 'cmam', 'XSepConv', 'DOConv', 'XSepDOConv'\n",
    "\n",
    "    output_dir=base+\"outputs\", \n",
    "    save_dir=base+\"model_storage\",\n",
    "    model_state_file=\"DOConv_latest_april1.pth\",\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "handle_dirs(args.save_dir)\n",
    "handle_dirs(args.output_dir)\n",
    "handle_dirs(args.preprocessed_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neKGz1KPdsch"
   },
   "source": [
    "# Data modification and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILhyQjHOdsci"
   },
   "source": [
    "### Creation of embedings\n",
    "Code from [ABAE GitHub](https://github.com/KirillKrasikov/Attention-And-Capsule-Based-Aspect-Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4aO6ePr6guhn",
    "outputId": "283747b8-f5e4-42f8-df2e-f2ee4e056bba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code from [ABAE GitHub](https://github.com/KirillKrasikov/Attention-And-Capsule-Based-Aspect-Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_G2C5rldsci",
    "outputId": "2ebc3d1b-fa57-4f81-d11d-05a792347773"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing raw review sentences ...\n",
      "Processing train data!\n",
      "Processing test data!\n",
      "Trainig Word2Vec model!\n",
      "Done preprocessing!\n"
     ]
    }
   ],
   "source": [
    "if not args.reload_from_files:\n",
    "    print('Preprocessing raw review sentences ...')\n",
    "    preprocess(args, args.raw_train_path, args.raw_test_path, args.raw_test_lab_path, args.preprocessed_fold)\n",
    "    print('Trainig Word2Vec model!')\n",
    "    word2vec(args.train_data, args.emb_path,  args.emb_dim, args.window, args.min_count)\n",
    "    print('Done preprocessing!')\n",
    "else:\n",
    "    print('Loading Preprocessing files and Word2Vec model from existing files!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "985dFqJv5mUY"
   },
   "source": [
    "# Training blocks (Jump from here to Evaluation)\n",
    "Don't run blocks below for evaluation part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-15Je7ddsci"
   },
   "source": [
    "### Data preprocessings (skip this block for evaluation)\n",
    "Code from [ABAE GitHub](https://github.com/KirillKrasikov/Attention-And-Capsule-Based-Aspect-Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jCwK4zLodsci"
   },
   "outputs": [],
   "source": [
    "#Code from [ABAE GitHub](https://github.com/KirillKrasikov/Attention-And-Capsule-Based-Aspect-Extraction)\n",
    "# sentences = []\n",
    "# with open(args.train_data) as fp:\n",
    "#     for line in fp.readlines():\n",
    "#         sentences.append(line)\n",
    "# cleaned_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
    "# train_df = pd.DataFrame(cleaned_sentences, columns=[\"sentence\"])\n",
    "\n",
    "# sentences = []\n",
    "# labels = []\n",
    "# with open(args.test_data) as fp:\n",
    "#     for line in fp.readlines():\n",
    "#         sentences.append(line)\n",
    "# with open(args.test_labels) as fp:\n",
    "#     for line in fp.readlines():\n",
    "#         labels.append(line)\n",
    "# cleaned_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
    "# cleaned_labels = [preprocess_text(label.split()[0]) for label in labels]\n",
    "# test_df = pd.DataFrame({'sentence': cleaned_sentences, 'label': cleaned_labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code from [ABAE GitHub](https://github.com/KirillKrasikov/Attention-And-Capsule-Based-Aspect-Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "vDmJMYORdscj"
   },
   "outputs": [],
   "source": [
    "#Code from [ABAE GitHub](https://github.com/KirillKrasikov/Attention-And-Capsule-Based-Aspect-Extraction)\n",
    "# w2v = gensim.models.Word2Vec.load(args.emb_path)\n",
    "# token2index_lim = {token: index for index, token in enumerate(w2v.wv.index2word) if index < args.vocab_size}\n",
    "# token2index_all = {token: index for index, token in enumerate(w2v.wv.index2word)}\n",
    "# vocab = Vocabulary(token2index_lim)\n",
    "# vectorizer = Vectorizer(vocab)\n",
    "# dataset = Dataset(train_df, test_df, vectorizer, max_length = args.maxlen )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kn-_Ypfadscj"
   },
   "source": [
    "### Model initialization (skip this block for evaluation)\n",
    "Code from [ABAE GitHub](https://github.com/KirillKrasikov/Attention-And-Capsule-Based-Aspect-Extraction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kNPUCtXudscj",
    "outputId": "10fd217f-0ee5-47e2-8ea0-4647164daee9"
   },
   "outputs": [],
   "source": [
    "# model = ABAE(\n",
    "#     wv_dim=args.emb_dim,\n",
    "#     asp_count=args.aspect_size,\n",
    "#     maxlen=dataset.max_seq_length, \n",
    "#     init_aspects_matrix=get_centroids(w2v, args.aspect_size),\n",
    "#     pretrained_embedding=w2v.wv.vectors,\n",
    "#     padding_index=vocab.mask_index,\n",
    "#     attention_mech=args.attention_mech\n",
    "# )\n",
    "\n",
    "# model = model.to(args.device)\n",
    "# # Loss funcs\n",
    "# loss_func = nn.TripletMarginLoss(margin=args.tripletmargin, swap=False, reduction='mean')\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer=optimizer,\n",
    "#     mode='min', \n",
    "#     factor=0.5,\n",
    "#     patience=1\n",
    "# )\n",
    "\n",
    "# train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nWJ5-zVgUZm"
   },
   "source": [
    "## Model Training (skip this block for evaluation)\n",
    "Code from [ABAE GitHub](https://github.com/KirillKrasikov/Attention-And-Capsule-Based-Aspect-Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "a140be3aa96444f489f5d4a76ada8051",
      "4443aa84cc084160845357cadf3c5c85",
      "9a09e3fa2e2344378b7165a17b35110c",
      "db959826fd7e4e28bf24b218bcfdb4f1",
      "2dbd19e07cc64ba9ac40d02029b4095d",
      "83e16d46cfb04ec98b9bac550ecfb3ad",
      "828475914f244991a826e23ba9c963bc",
      "c033d82238d34b22b1c735416e2bf8af",
      "a77731c304db45ea9eafddde0312e727",
      "1acb81a0577e4b6994f7c68144952813",
      "b4e9d3daf876419e8b8e0e252ebd595a",
      "aa3b7a836c4d4eb7abdd42311aa794a9",
      "2a76b86992fa469fad0410b5632c787d",
      "df3d62b16277403eb2e0436cc00bd56f",
      "00ef6eefe55d4242906b57a71293e56f",
      "51fb662626ae4208a1ce541c2a33587c"
     ]
    },
    "id": "W3m74DWadsck",
    "outputId": "ee361af7-a927-411c-abb3-60a50c0501a3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Code from [ABAE GitHub](https://github.com/KirillKrasikov/Attention-And-Capsule-Based-Aspect-Extraction)\n",
    "\n",
    "# epoch_bar = tqdm(\n",
    "#     desc='training routine', \n",
    "#     total=args.epochs,\n",
    "#     position=1,\n",
    "# )\n",
    "\n",
    "# dataset.set_split('train')\n",
    "# train_bar = tqdm(\n",
    "#     desc='train',\n",
    "#     total=dataset.get_num_batches(args.batch_size), \n",
    "#     position=1, \n",
    "# )\n",
    "\n",
    "# y = torch.zeros(args.batch_size, 1)\n",
    "\n",
    "# for epoch_index in range(args.epochs):\n",
    "    \n",
    "#     train_state['epoch_index'] = epoch_index\n",
    "    \n",
    "#     running_loss = 0.0\n",
    "#     running_loss1, running_loss2, running_loss3 = 0.0, 0.0, 0.0\n",
    "#     model.train()\n",
    "    \n",
    "#     batch_generator = generate_batches(\n",
    "#         dataset, \n",
    "#         batch_size=args.batch_size, \n",
    "#         device=args.device\n",
    "#     )\n",
    "    \n",
    "#     neg_batch_generator = generate_batches(\n",
    "#         dataset, \n",
    "#         batch_size=args.batch_size, \n",
    "#         shuffle=False,\n",
    "#         device=args.device,\n",
    "#     )\n",
    "\n",
    "#     for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         x = batch_dict['x_data']\n",
    "#         y = batch_dict['y_target'].float()\n",
    "#         x_neg = next(neg_batch_generator)['x_data']\n",
    "\n",
    "#         negative_samples = torch.stack(\n",
    "#             tuple([x_neg[torch.randperm(x_neg.shape[0])[:args.neg_size]] \n",
    "#                    for _ in range(args.batch_size)])\n",
    "#         ).to(args.device)\n",
    "\n",
    "#         anchor, positive, negative, asp_emb = model(x, negative_samples)\n",
    "\n",
    "#         loss = loss_func(anchor, positive, negative)       \n",
    "#         loss_t = loss.item()\n",
    "#         running_loss += (loss_t - running_loss) / (batch_index + 1)    \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         train_bar.set_postfix(loss=running_loss,  epoch=epoch_index) #Ortogonal=running_loss2,\n",
    "#         train_bar.update()\n",
    "\n",
    "#     train_state['train_loss'].append(running_loss)\n",
    "#     train_state = update_train_state(args=args, model=model, train_state=train_state)\n",
    "#     scheduler.step(train_state['train_loss'][-1])\n",
    "    \n",
    "#     # uncomment the lines below to display the loss and aspects words after each training loop\n",
    "#     print(\"epoch {}, batches {}, loss {:.5f}, and LR {}:\".format(epoch_index, batch_index, running_loss, optimizer.param_groups[0]['lr']))\n",
    "#     # uncomment the lines below to display aspects words after each training loop\n",
    "#     for i, aspect in enumerate(model.get_aspect_words(w2v)):\n",
    "#         print(i, \" \".join([a for a in aspect]))\n",
    "#     print()\n",
    "    \n",
    "\n",
    "#     if train_state['stop_early']:\n",
    "#         break\n",
    "\n",
    "#     train_bar.n = 0\n",
    "\n",
    "#     epoch_bar.set_postfix(best_val=train_state['early_stopping_best_val'])\n",
    "#     epoch_bar.update()\n",
    "\n",
    "# # save aspect words\n",
    "# save_aspect_words(model, args, topn=100)\n",
    "\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# sns.lineplot(\n",
    "#     x=[epoch + 1 for epoch in range(len(train_state['train_loss']))],\n",
    "#     y=train_state['train_loss'],\n",
    "#     color='coral', \n",
    "#     label='loss',\n",
    "# )\n",
    "\n",
    "# plt.xticks([epoch for epoch in range(len(train_state['train_loss']) + 1)])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRpm-1zCdscm"
   },
   "source": [
    "# Evaluation\n",
    "Code from [ABAE GitHub](https://github.com/KirillKrasikov/Attention-And-Capsule-Based-Aspect-Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "JgXxURJMyX6-"
   },
   "outputs": [],
   "source": [
    "#Code from [ABAE GitHub](https://github.com/KirillKrasikov/Attention-And-Capsule-Based-Aspect-Extraction) with rearanging code\n",
    " \n",
    "sentences = []\n",
    "with open(args.train_data) as fp:\n",
    "    for line in fp.readlines():\n",
    "        sentences.append(line)\n",
    "cleaned_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
    "train_df = pd.DataFrame(cleaned_sentences, columns=[\"sentence\"])\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "with open(args.test_data) as fp:\n",
    "    for line in fp.readlines():\n",
    "        sentences.append(line)\n",
    "with open(args.test_labels) as fp:\n",
    "    for line in fp.readlines():\n",
    "        labels.append(line)\n",
    "cleaned_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
    "cleaned_labels = [preprocess_text(label.split()[0]) for label in labels]\n",
    "\n",
    "test_df = pd.DataFrame({'sentence': cleaned_sentences, 'label': cleaned_labels})\n",
    "\n",
    "w2v = gensim.models.Word2Vec.load(args.emb_path)\n",
    "token2index_lim = {token: index for index, token in enumerate(w2v.wv.index2word) if index < args.vocab_size}\n",
    "token2index_all = {token: index for index, token in enumerate(w2v.wv.index2word)}\n",
    "vocab = Vocabulary(token2index_lim)\n",
    "vectorizer = Vectorizer(vocab)\n",
    "dataset = Dataset(train_df, test_df, vectorizer, max_length = args.maxlen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code from [ABAE GitHub](https://github.com/KirillKrasikov/Attention-And-Capsule-Based-Aspect-Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RUJ8BVlHnDEi",
    "outputId": "7d34ede7-0524-451d-8fd2-739be7822016"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "===================================\n",
      "Training/Evaluating using DOConv\n",
      "=============================================\n",
      "0 atmosphere cozy setting romantic lighting intimate space decor comfortable ambience\n",
      "1 review experience expectation anniversary filet birthday dined bass rating reviewer\n",
      "2 sauce dish tomato mushroom chicken grilled vegetable roasted shrimp flavor\n",
      "3 wine fixe prix bottle selection excellent great entree beer appetizer\n",
      "4 city nyc best brooklyn ny manhattan lived park york favorite\n",
      "5 pork lamb lobster grilled tuna shrimp rib recommend beef tomato\n",
      "6 dry fry bland tasteless salty burnt overpriced onion greasy crust\n",
      "7 food cuisine price quality fare sushi value priced authentic portion\n",
      "8 wall ceiling wood chair window glass white red booth lit\n",
      "9 staff hostess waiter server waitstaff manager waitress bartender service rude\n",
      "10 go wanted want decided someone try anyone going friend eat\n",
      "11 table minute u asked seated seat reservation min told waited\n",
      "12 saturday friday night weekend went sunday early afternoon reservation late\n",
      "13 potato chocolate banana mango lemon cream butter coconut creme creamy\n"
     ]
    }
   ],
   "source": [
    "#Code from [ABAE GitHub](https://github.com/KirillKrasikov/Attention-And-Capsule-Based-Aspect-Extraction)\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "model = ABAE(\n",
    "    wv_dim=args.emb_dim,\n",
    "    asp_count=args.aspect_size,\n",
    "    maxlen=dataset.max_seq_length,\n",
    "    init_aspects_matrix=get_centroids(w2v, args.aspect_size),\n",
    "    pretrained_embedding=w2v.wv.vectors,\n",
    "    padding_index=vocab.mask_index,\n",
    "    attention_mech=args.attention_mech\n",
    ")\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "model.load_state_dict(torch.load(train_state['model_filename'], map_location=args.device))\n",
    "model = model.to(args.device)\n",
    "loss_func = torch.nn.MSELoss(reduction=\"sum\")\n",
    "dataset.set_split('test')\n",
    "\n",
    "print('=============================================')\n",
    "for i, aspect in enumerate(model.get_aspect_words(w2v)):\n",
    "        print(i, \" \".join([a for a in aspect]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code from [ABAE GitHub](https://github.com/KirillKrasikov/Attention-And-Capsule-Based-Aspect-Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 atmosphere cozy setting romantic lighting intimate space decor comfortable ambience\n",
      "1 review experience expectation anniversary filet birthday dined bass rating reviewer\n",
      "2 sauce dish tomato mushroom chicken grilled vegetable roasted shrimp flavor\n",
      "3 wine fixe prix bottle selection excellent great entree beer appetizer\n",
      "4 city nyc best brooklyn ny manhattan lived park york favorite\n",
      "5 pork lamb lobster grilled tuna shrimp rib recommend beef tomato\n",
      "6 dry fry bland tasteless salty burnt overpriced onion greasy crust\n",
      "7 food cuisine price quality fare sushi value priced authentic portion\n",
      "8 wall ceiling wood chair window glass white red booth lit\n",
      "9 staff hostess waiter server waitstaff manager waitress bartender service rude\n",
      "10 go wanted want decided someone try anyone going friend eat\n",
      "11 table minute u asked seated seat reservation min told waited\n",
      "12 saturday friday night weekend went sunday early afternoon reservation late\n",
      "13 potato chocolate banana mango lemon cream butter coconut creme creamy\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame()\n",
    "for i, aspect in enumerate(model.get_aspect_words(w2v)):\n",
    "        print(i, \" \".join([a for a in aspect]))\n",
    "        df1 = df1.append({'Aspects' : \" \".join([a for a in aspect])}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip text\n",
    "df1['Colum']= df1['Aspects'].apply(lambda x: [x for x in x.strip(\" \").lower().split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#considered entities\n",
    "entity =['food','staff','ambience']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vectors directly from the file\n",
    "model1 = KeyedVectors.load_word2vec_format('pretrained_model/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "model_vocab = list(model1.vocab.keys())\n",
    "result1=[]\n",
    "result3=[]\n",
    "result4=[]\n",
    "dict_scores = {}\n",
    "for ii in range(len(df1)):    \n",
    "    for j in range(10):\n",
    "        for i in entity:\n",
    "            pos = i\n",
    "            if df1.Colum[ii][j] in model_vocab:\n",
    "                result = model1.similarity(pos,df1.Colum[ii][j])\n",
    "                result3.append(((result,pos,df1.Colum[ii][j])))\n",
    "                result4.append(((result,pos,df1.Colum[ii][j])))\n",
    "    dict_scores[ii]=result4\n",
    "    result4=[]\n",
    "        #result4.append(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******Food***********\n",
      "{0: 0.1510377686470747, 1: 0.08288275175727904, 2: 0.2982382267713547, 3: 0.1790340581908822, 4: 0.07333283429034054, 5: 0.29235954619944093, 6: 0.20208619087934493, 7: 0.29501340016722677, 8: 0.059674300532788035, 9: 0.1733293980360031, 10: 0.1280304319690913, 11: 0.062248680368065835, 12: 0.05398568734526634, 13: 0.20696413442492484}\n",
      "*******Staff**********\n",
      "{0: 0.1036470353603363, 1: 0.09852286404930055, 2: 0.02783063028473407, 3: 0.03218751852982678, 4: 0.029503638856112957, 5: 0.05186868025921285, 6: 0.026320598064921798, 7: 0.07108102194033564, 8: 0.06529045244678855, 9: 0.28852002173662183, 10: 0.09105376675724983, 11: 0.07523254407569765, 12: 0.05683073757681996, 13: 0.013206278797588311}\n",
      "*******Amb***********\n",
      "{0: 0.4129987359046936, 1: 0.14091617222875358, 2: 0.17418329119682313, 3: 0.16672823280096055, 4: 0.11718497835099698, 5: 0.07533262809738517, 6: 0.16679588481783866, 7: 0.22692797444760798, 8: 0.10966069282731042, 9: 0.18574624881148338, 10: 0.08086255453526973, 11: 0.059674665238708256, 12: 0.050111198984086516, 13: 0.14396060183644294}\n"
     ]
    }
   ],
   "source": [
    "food=0\n",
    "staf=0\n",
    "amb=0\n",
    "food_dictionary={}\n",
    "staf_dictionary={}\n",
    "amb_dictionary={}\n",
    "\n",
    "j=0\n",
    "for d in dict_scores.values():\n",
    "    #print(d)\n",
    "    \n",
    "    food_count = 0\n",
    "    food=0\n",
    "    staf=0\n",
    "    amb=0\n",
    "    for i in range(len(d)):\n",
    "        if(i%3==0):\n",
    "            food+=d[i][0]\n",
    "            food_count+=1\n",
    "            staf+=d[i+1][0]            \n",
    "            amb+=d[i+2][0]\n",
    "\n",
    "            \n",
    "    food_score=food/food_count\n",
    "    food_dictionary[j]=food_score\n",
    "    \n",
    "    staf_score=staf/food_count\n",
    "    staf_dictionary[j]=staf_score\n",
    "    \n",
    "    amb_score=amb/food_count\n",
    "    amb_dictionary[j]=amb_score\n",
    "    j+=1\n",
    "print(\"*******Food***********\")\n",
    "print(food_dictionary)\n",
    "print(\"*******Staff**********\")\n",
    "print(staf_dictionary)\n",
    "print(\"*******Amb***********\")\n",
    "print(amb_dictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Code from [ABAE GitHub](https://github.com/KirillKrasikov/Attention-And-Capsule-Based-Aspect-Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yvG_VhlegUZo",
    "outputId": "13efdc9b-b04f-4ac4-a646-834d4be2ed46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    ambience     0.7737    0.7490    0.7611       251\n",
      "        food     0.8659    0.9538    0.9077       887\n",
      "       staff     0.8926    0.6847    0.7749       352\n",
      "\n",
      "    accuracy                         0.8557      1490\n",
      "   macro avg     0.8441    0.7958    0.8146      1490\n",
      "weighted avg     0.8567    0.8557    0.8517      1490\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Code from [ABAE GitHub](https://github.com/KirillKrasikov/Attention-And-Capsule-Based-Aspect-Extraction)\n",
    "# ignores aspect clusters\n",
    "def ignoreFunction(aspect_probs, cluster_map): \n",
    "    def get_label(lst, aspect_gold=cluster_map):\n",
    "        idxs = np.argsort(np.array(lst))[::-1]\n",
    "        i = 0\n",
    "        while cluster_map[idxs[i]] =='ignore':\n",
    "            i += 1\n",
    "        return idxs[i]\n",
    "    copy = aspect_probs.copy()\n",
    "    label_ids = np.apply_along_axis(get_label, 1, copy)\n",
    "    return label_ids.astype('int')\n",
    "\n",
    "\n",
    "model.eval()\n",
    "model.encoder_only = True\n",
    "aspect_probs = []\n",
    "predictions = []\n",
    "targets = []\n",
    "word_weights = np.empty((0,dataset.max_seq_length))\n",
    "\n",
    "batch_generator = generate_batches(\n",
    "    dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    device=args.device,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "        x = batch_dict['x_data']\n",
    "        y_target = batch_dict['y_target']\n",
    "\n",
    "        y_pred, word_weights_batch = model(x, None)\n",
    "        word_weights = np.concatenate((word_weights, word_weights_batch.cpu().numpy()), axis=0)\n",
    "\n",
    "        for pred, target in zip(y_pred, y_target):\n",
    "            aspect_probs.append(pred.cpu().numpy())\n",
    "#             predictions.append(pred.cpu().numpy().argmax())\n",
    "            targets.append(target)\n",
    "\n",
    "_ = ['ambience', 'food', 'miscellaneous', 'price', 'staff', 'anecdotes', 'ignore']\n",
    "\n",
    "cluster_map = {\n",
    "    0: 'ambience', \n",
    "    1: 'ignore', \n",
    "    2: 'food', \n",
    "    3: 'ignore',\n",
    "    4: 'ignore', \n",
    "    5: 'food', \n",
    "    6: 'food',  \n",
    "    7: 'food',\n",
    "    8: 'ignore', \n",
    "    9: 'staff', \n",
    "    10: 'ignore', \n",
    "    11: 'ignore', \n",
    "    12: 'ignore', \n",
    "    13: 'food'\n",
    "}\n",
    "\n",
    "predictions = ignoreFunction(aspect_probs, cluster_map)\n",
    "y_pred = [cluster_map[pred] for pred in predictions]\n",
    "y_true = targets\n",
    "print(classification_report(y_true, y_pred, digits=4 , labels=np.unique(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "056AENA-bvDP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00ef6eefe55d4242906b57a71293e56f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1acb81a0577e4b6994f7c68144952813": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a76b86992fa469fad0410b5632c787d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "2dbd19e07cc64ba9ac40d02029b4095d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "4443aa84cc084160845357cadf3c5c85": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51fb662626ae4208a1ce541c2a33587c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "828475914f244991a826e23ba9c963bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "83e16d46cfb04ec98b9bac550ecfb3ad": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a09e3fa2e2344378b7165a17b35110c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "training routine: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83e16d46cfb04ec98b9bac550ecfb3ad",
      "max": 15,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2dbd19e07cc64ba9ac40d02029b4095d",
      "value": 15
     }
    },
    "a140be3aa96444f489f5d4a76ada8051": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9a09e3fa2e2344378b7165a17b35110c",
       "IPY_MODEL_db959826fd7e4e28bf24b218bcfdb4f1"
      ],
      "layout": "IPY_MODEL_4443aa84cc084160845357cadf3c5c85"
     }
    },
    "a77731c304db45ea9eafddde0312e727": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b4e9d3daf876419e8b8e0e252ebd595a",
       "IPY_MODEL_aa3b7a836c4d4eb7abdd42311aa794a9"
      ],
      "layout": "IPY_MODEL_1acb81a0577e4b6994f7c68144952813"
     }
    },
    "aa3b7a836c4d4eb7abdd42311aa794a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_51fb662626ae4208a1ce541c2a33587c",
      "placeholder": "",
      "style": "IPY_MODEL_00ef6eefe55d4242906b57a71293e56f",
      "value": " 5596/5597 [30:55&lt;00:00, 45.58it/s, epoch=14, loss=0.0848]"
     }
    },
    "b4e9d3daf876419e8b8e0e252ebd595a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "train: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df3d62b16277403eb2e0436cc00bd56f",
      "max": 5597,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2a76b86992fa469fad0410b5632c787d",
      "value": 5596
     }
    },
    "c033d82238d34b22b1c735416e2bf8af": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db959826fd7e4e28bf24b218bcfdb4f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c033d82238d34b22b1c735416e2bf8af",
      "placeholder": "",
      "style": "IPY_MODEL_828475914f244991a826e23ba9c963bc",
      "value": " 15/15 [30:56&lt;00:00, 125.33s/it, best_val=0.0848]"
     }
    },
    "df3d62b16277403eb2e0436cc00bd56f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
